{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":9542468,"sourceType":"datasetVersion","datasetId":5813202}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport numpy as np\nfrom moviepy.editor import VideoFileClip\nfrom tqdm.notebook import tqdm\nfrom typing import List, Tuple, Dict, Union\nimport matplotlib.pyplot as plt\nimport pickle as pkl","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:17:01.053315Z","iopub.execute_input":"2024-10-03T23:17:01.054003Z","iopub.status.idle":"2024-10-03T23:17:02.281391Z","shell.execute_reply.started":"2024-10-03T23:17:01.053955Z","shell.execute_reply":"2024-10-03T23:17:02.280442Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install moviepy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Preprocessing Data","metadata":{}},{"cell_type":"code","source":"dataset_path = '/kaggle/input/shoplifting-dataset/shop_dataset'\nnon_shop_lifters_path = os.path.join(dataset_path, 'non shop lifters')\nshop_lifters_path = os.path.join(dataset_path, 'shop lifters')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:17:03.699431Z","iopub.execute_input":"2024-10-03T23:17:03.699935Z","iopub.status.idle":"2024-10-03T23:17:03.704887Z","shell.execute_reply.started":"2024-10-03T23:17:03.699896Z","shell.execute_reply":"2024-10-03T23:17:03.703936Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"len(os.listdir(non_shop_lifters_path)), len(os.listdir(shop_lifters_path))","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:17:03.801250Z","iopub.execute_input":"2024-10-03T23:17:03.801852Z","iopub.status.idle":"2024-10-03T23:17:03.873588Z","shell.execute_reply.started":"2024-10-03T23:17:03.801811Z","shell.execute_reply":"2024-10-03T23:17:03.872648Z"},"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"(531, 324)"},"metadata":{}}]},{"cell_type":"code","source":"def get_frame_rate(video_path):\n    # Open the video file\n    video = cv2.VideoCapture(video_path)\n    \n    # Check if the video was opened successfully\n    if not video.isOpened():\n        print(f\"Error: Cannot open video file {video_path}\")\n        return None\n    \n    # Get the frame rate\n    fps = video.get(cv2.CAP_PROP_FPS)\n    \n    # Release the video object\n    video.release()\n    \n    return fps","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:06:43.801924Z","iopub.execute_input":"2024-10-03T23:06:43.802656Z","iopub.status.idle":"2024-10-03T23:06:43.808189Z","shell.execute_reply.started":"2024-10-03T23:06:43.802612Z","shell.execute_reply":"2024-10-03T23:06:43.807137Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"get_frame_rate(f'{dataset_path}/non shop lifters/shop_lifter_n_1.mp4')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"get_frame_rate(f'{dataset_path}/shop lifters/shop_lifter_10.mp4')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def process_and_extract_frames(\n        dataset_path: str, \n        new_dir_path: str=f'/kaggle/working/preprocessed_data', \n        target_shape: Tuple[int,int]=(224,224),\n        sample_rate: int=1):\n    \"\"\"\n    Processes and extracts frames from videos in the dataset path.\n\n    Parameters:\n    - dataset_path (str): Path to the folder containing video files.\n    - new_dir_path (str): Path to save the processed .npy files. Default is './Shop DataSet/preprocessed_data'.\n    - target_shape (tuple): Desired frame size for resizing. Default is (224, 224).\n    - sample_rate (int): Interval to sample frames. Default is 1 (every frame).\n\n    \"\"\"\n    new_path = os.path.join(new_dir_path, dataset_path.split('/')[-1])\n    os.makedirs(new_path, exist_ok=True)\n    for video in tqdm(os.listdir(dataset_path)):\n        video_path = os.path.join(dataset_path, video)\n        cap = cv2.VideoCapture(video_path)\n        frames = []\n        frame_count = 0\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            if frame_count % sample_rate == 0:\n                frame = cv2.resize(frame, target_shape)\n                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n                frames.append(frame)\n            frame_count += 1\n        cap.release()\n        frames = np.array(frames)\n        output_filename = f\"{video.split('.')[0]}.pkl\"\n        # np.savez_compressed(os.path.join(new_path, output_filename), frames)\n        # save as pkl file\n        pkl.dump(frames, open(os.path.join(new_path, output_filename), 'wb'))\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"process_and_extract_frames(shop_lifters_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"process_and_extract_frames(non_shop_lifters_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"new_dataset_path = '/kaggle/working/preprocessed_data'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_video = pkl.load(open(f'{new_dataset_path}/shop lifters/shop_lifter_0.pkl', 'rb'))\nsample_video.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def get_seq_len(dataset_path: str) -> Tuple[int,int]:\n    min_seq_len = float('inf')\n    max_seq_len = float('-inf')\n    all_seq_len = []\n    for video_path in tqdm(os.listdir(dataset_path)):\n        video = pkl.load(open(os.path.join(dataset_path, video_path), 'rb'))\n        min_seq_len = min(min_seq_len, video.shape[0])\n        max_seq_len = max(max_seq_len, video.shape[0])\n        all_seq_len.append(video.shape[0])\n    return min_seq_len, max_seq_len, all_seq_len","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"min_sl_seq_len, max_sl_seq_len, all_sl_seq_len = get_seq_len(f'{new_dataset_path}/shop lifters')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"min_sl_seq_len, max_sl_seq_len","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"min_nsl_seq_len, max_nsl_seq_len, all_nsl_seq_len = get_seq_len('./Shop DataSet/preprocessed_data/non shop lifters')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"min_nsl_seq_len, max_nsl_seq_len","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_seq_len = all_sl_seq_len + all_nsl_seq_len\nall_seq_len = np.array(all_seq_len)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(10, 6))\nplt.hist(all_seq_len, bins=30, color='skyblue', edgecolor='black')\nplt.title('Distribution of Video Sequence Lengths')\nplt.xlabel('Sequence Length')\nplt.ylabel('Number of Videos')\nplt.grid(True)\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_seq_len.min(), all_seq_len.max()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.percentile(a=all_seq_len, q=97)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Prepare Dataset","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nimport os\n\nshop_lifter_pkl = os.listdir(f'{dataset_path}/shop lifters')\nnon_shop_lifter_pkl = os.listdir(f'{dataset_path}/non shop lifters')\n# non_shop_lifter_pkl = [file for file in non_shop_lifter_pkl if ('_1' not in file) or (file == 'shop_lifter_n_1.pkl')]\n\nshop_lifter_pkl = [f'{dataset_path}/shop lifters/{file}' for file in shop_lifter_pkl]\nnon_shop_lifter_pkl = [f'{dataset_path}/non shop lifters/{file}' for file in non_shop_lifter_pkl]\n\nall_pkl = shop_lifter_pkl + non_shop_lifter_pkl\nlabels = [1] * len(shop_lifter_pkl) + [0] * len(non_shop_lifter_pkl)\n\ntrain_files, temp_files, train_labels, temp_labels = train_test_split(\n    all_pkl, labels, test_size=0.15, random_state=42, shuffle=True, stratify=labels\n)\n\n# Split the temporary set into validation and test set with stratification\nval_files, test_files, val_labels, test_labels = train_test_split(\n    temp_files, temp_labels, test_size=0.5, random_state=42, shuffle=True, stratify=temp_labels\n)\n\n# Print the count of each class in each set for verification\nprint(f'Training set: {len(train_files)} files, {sum(train_labels)} shop lifters, {len(train_labels) - sum(train_labels)} non-shop lifters')\nprint(f'Validation set: {len(val_files)} files, {sum(val_labels)} shop lifters, {len(val_labels) - sum(val_labels)} non-shop lifters')\nprint(f'Test set: {len(test_files)} files, {sum(test_labels)} shop lifters, {len(test_labels) - sum(test_labels)} non-shop lifters')","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:17:07.452659Z","iopub.execute_input":"2024-10-03T23:17:07.453056Z","iopub.status.idle":"2024-10-03T23:17:08.307847Z","shell.execute_reply.started":"2024-10-03T23:17:07.453015Z","shell.execute_reply":"2024-10-03T23:17:08.306810Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Training set: 726 files, 275 shop lifters, 451 non-shop lifters\nValidation set: 64 files, 24 shop lifters, 40 non-shop lifters\nTest set: 65 files, 25 shop lifters, 40 non-shop lifters\n","output_type":"stream"}]},{"cell_type":"code","source":"import numpy as np\nfrom sklearn.utils import resample\n\n# Separate the training files and labels into the two classes\nshop_lifter_train = [file for file, label in zip(train_files, train_labels) if label == 1]\nnon_shop_lifter_train = [file for file, label in zip(train_files, train_labels) if label == 0]\n\n# Undersample the non-shop lifters to match the number of shop lifters\nnon_shop_lifter_train_balanced = resample(non_shop_lifter_train,\n                                          replace=False,\n                                          n_samples=len(shop_lifter_train),\n                                          random_state=42)\n\n# Combine the balanced datasets\ntrain_files_balanced = shop_lifter_train + non_shop_lifter_train_balanced\ntrain_files_balanced = np.array(train_files_balanced)\nshuffled_indices = np.random.permutation(len(train_files_balanced))\ntrain_files_balanced = train_files_balanced[shuffled_indices].tolist()\n\n# Print the balanced training set counts\nprint(f'Balanced Training set: {len(train_files_balanced)} files, '\n      f'{len(shop_lifter_train)} shop lifters, '\n      f'{len(non_shop_lifter_train_balanced)} non-shop lifters')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:17:08.309468Z","iopub.execute_input":"2024-10-03T23:17:08.309904Z","iopub.status.idle":"2024-10-03T23:17:08.319637Z","shell.execute_reply.started":"2024-10-03T23:17:08.309866Z","shell.execute_reply":"2024-10-03T23:17:08.318516Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Balanced Training set: 550 files, 275 shop lifters, 275 non-shop lifters\n","output_type":"stream"}]},{"cell_type":"code","source":"len(shop_lifter_pkl), len(non_shop_lifter_pkl)","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:17:08.321317Z","iopub.execute_input":"2024-10-03T23:17:08.321711Z","iopub.status.idle":"2024-10-03T23:17:08.333633Z","shell.execute_reply.started":"2024-10-03T23:17:08.321665Z","shell.execute_reply":"2024-10-03T23:17:08.332612Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(324, 531)"},"metadata":{}}]},{"cell_type":"code","source":"len(train_files_balanced), len(val_files), len(test_files)","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:17:08.335578Z","iopub.execute_input":"2024-10-03T23:17:08.335880Z","iopub.status.idle":"2024-10-03T23:17:08.344592Z","shell.execute_reply.started":"2024-10-03T23:17:08.335847Z","shell.execute_reply":"2024-10-03T23:17:08.343758Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(550, 64, 65)"},"metadata":{}}]},{"cell_type":"code","source":"from typing import List, Dict\ndef get_sl_and_nsl(files: List[str]) -> Dict[int,int]:\n    sl = 0\n    nsl = 0\n    for f in files:\n        if 'non shop lifters' in f:\n            nsl += 1\n        else:\n            sl += 1\n    print(f'Shop Lifters: {sl}, Non Shop Lifters: {nsl}')\n    return {\n        0: nsl,\n        1: sl\n    }","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:17:08.345701Z","iopub.execute_input":"2024-10-03T23:17:08.346060Z","iopub.status.idle":"2024-10-03T23:17:08.355434Z","shell.execute_reply.started":"2024-10-03T23:17:08.346016Z","shell.execute_reply":"2024-10-03T23:17:08.352994Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"_ = get_sl_and_nsl(train_files_balanced)","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:17:08.496105Z","iopub.execute_input":"2024-10-03T23:17:08.496995Z","iopub.status.idle":"2024-10-03T23:17:08.501873Z","shell.execute_reply.started":"2024-10-03T23:17:08.496955Z","shell.execute_reply":"2024-10-03T23:17:08.500871Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"Shop Lifters: 275, Non Shop Lifters: 275\n","output_type":"stream"}]},{"cell_type":"code","source":"_ = get_sl_and_nsl(test_files)","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:17:08.636228Z","iopub.execute_input":"2024-10-03T23:17:08.636636Z","iopub.status.idle":"2024-10-03T23:17:08.641629Z","shell.execute_reply.started":"2024-10-03T23:17:08.636598Z","shell.execute_reply":"2024-10-03T23:17:08.640593Z"},"trusted":true},"execution_count":10,"outputs":[{"name":"stdout","text":"Shop Lifters: 25, Non Shop Lifters: 40\n","output_type":"stream"}]},{"cell_type":"code","source":"_ = get_sl_and_nsl(val_files)","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:17:08.784764Z","iopub.execute_input":"2024-10-03T23:17:08.785199Z","iopub.status.idle":"2024-10-03T23:17:08.790459Z","shell.execute_reply.started":"2024-10-03T23:17:08.785158Z","shell.execute_reply":"2024-10-03T23:17:08.789465Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"Shop Lifters: 24, Non Shop Lifters: 40\n","output_type":"stream"}]},{"cell_type":"code","source":"def calculate_class_weights(files: List[str]) -> Dict[int, float]:\n    class_weights = {}\n    class_count = get_sl_and_nsl(files)\n    total_samples = sum(class_count.values())\n    for cls, count in class_count.items():\n        class_weights[cls] = total_samples / count\n    normalized_class_weights = {}\n    sum_class_weights = sum(class_weights.values())\n    for cls, weight in class_weights.items():\n        normalized_class_weights[cls] = weight / sum_class_weights\n    return class_weights, normalized_class_weights","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:17:08.957331Z","iopub.execute_input":"2024-10-03T23:17:08.958054Z","iopub.status.idle":"2024-10-03T23:17:08.964425Z","shell.execute_reply.started":"2024-10-03T23:17:08.958009Z","shell.execute_reply":"2024-10-03T23:17:08.963437Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"train_class_weights, normalized_class_weights = calculate_class_weights(train_files_balanced)","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:17:09.066875Z","iopub.execute_input":"2024-10-03T23:17:09.068050Z","iopub.status.idle":"2024-10-03T23:17:09.072810Z","shell.execute_reply.started":"2024-10-03T23:17:09.068006Z","shell.execute_reply":"2024-10-03T23:17:09.071894Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Shop Lifters: 275, Non Shop Lifters: 275\n","output_type":"stream"}]},{"cell_type":"code","source":"train_class_weights, normalized_class_weights","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:17:09.189170Z","iopub.execute_input":"2024-10-03T23:17:09.190070Z","iopub.status.idle":"2024-10-03T23:17:09.195847Z","shell.execute_reply.started":"2024-10-03T23:17:09.190030Z","shell.execute_reply":"2024-10-03T23:17:09.194857Z"},"trusted":true},"execution_count":14,"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"({0: 2.0, 1: 2.0}, {0: 0.5, 1: 0.5})"},"metadata":{}}]},{"cell_type":"code","source":"from torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms as T\nimport torch\nimport pickle as pkl\nimport random\nimport numpy as np\nfrom PIL import Image\nfrom typing import Tuple, List, Dict\nfrom transformers import AutoImageProcessor\n\n\n# def set_seed(seed):\n#     random.seed(seed)\n#     np.random.seed(seed)\n#     torch.manual_seed(seed)\n#     if torch.cuda.is_available():\n#         torch.cuda.manual_seed(seed)\n#         torch.cuda.manual_seed_all(seed)\n\nclass VideoDataset(Dataset):\n    def __init__(self, video_files: List[str], num_frames: int=32, ckpt_name: str='facebook/timesformer-base-finetuned-k400'):\n        self.video_files = video_files\n        self.num_frames = num_frames\n        self.processor = AutoImageProcessor.from_pretrained(ckpt_name)\n  \n        \n    def __len__(self):\n        return len(self.video_files)\n    \n    def __getitem__(self, idx):\n        video_path = self.video_files[idx]\n        video = self.extract_frames(video_path)\n        frame_indices = np.linspace(0, len(video)-1, self.num_frames, dtype=int)\n        video = [Image.fromarray(frame.astype('uint8')) for frame in video]\n        label = 0. if 'non shop lifters' in video_path else 1.\n        \n        video = [video[frame_idx] for frame_idx in frame_indices]\n        video = self.processor(video, return_tensors=\"pt\")\n        video['pixel_values'] = video['pixel_values'][0]\n        # print(video)\n        \n        return video , label\n    \n    def extract_frames(self, video_path: str):\n        cap = cv2.VideoCapture(video_path)\n        frames = []\n        while True:\n            ret, frame = cap.read()\n            if not ret:\n                break\n            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            frames.append(frame)\n        cap.release()\n        frames = np.array(frames)\n        return frames","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:17:09.929535Z","iopub.execute_input":"2024-10-03T23:17:09.929943Z","iopub.status.idle":"2024-10-03T23:17:24.385324Z","shell.execute_reply.started":"2024-10-03T23:17:09.929902Z","shell.execute_reply":"2024-10-03T23:17:24.384263Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"num_frames = 24\ntrain_dataset = VideoDataset(train_files_balanced, num_frames=num_frames)\nval_dataset = VideoDataset(val_files, num_frames=num_frames)\ntest_dataset = VideoDataset(test_files, num_frames=num_frames)","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:17:24.387299Z","iopub.execute_input":"2024-10-03T23:17:24.387955Z","iopub.status.idle":"2024-10-03T23:17:24.759780Z","shell.execute_reply.started":"2024-10-03T23:17:24.387913Z","shell.execute_reply":"2024-10-03T23:17:24.758800Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"for x in train_dataset:\n    print(x[0]['pixel_values'].shape, x[1])\n    break","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:17:24.761409Z","iopub.execute_input":"2024-10-03T23:17:24.761743Z","iopub.status.idle":"2024-10-03T23:17:25.444043Z","shell.execute_reply.started":"2024-10-03T23:17:24.761707Z","shell.execute_reply":"2024-10-03T23:17:25.442949Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"torch.Size([24, 3, 224, 224]) 1.0\n","output_type":"stream"}]},{"cell_type":"code","source":"train_dataloader = DataLoader(train_dataset, batch_size=2)\nval_dataloader = DataLoader(val_dataset, batch_size=1)\ntest_dataloader = DataLoader(test_dataset, batch_size=1)","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:17:25.446948Z","iopub.execute_input":"2024-10-03T23:17:25.447402Z","iopub.status.idle":"2024-10-03T23:17:25.452697Z","shell.execute_reply.started":"2024-10-03T23:17:25.447362Z","shell.execute_reply":"2024-10-03T23:17:25.451733Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"for x in train_dataloader:\n    print(x[0]['pixel_values'].shape, x[1])\n    break","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:17:25.454023Z","iopub.execute_input":"2024-10-03T23:17:25.454343Z","iopub.status.idle":"2024-10-03T23:17:27.665362Z","shell.execute_reply.started":"2024-10-03T23:17:25.454310Z","shell.execute_reply":"2024-10-03T23:17:27.664335Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"torch.Size([2, 24, 3, 224, 224]) tensor([1., 1.], dtype=torch.float64)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"\nfrom transformers import AutoModelForVideoClassification\nimport torch.nn as nn\n\nclass HFModel(nn.Module):\n    def __init__(self, ckpt_name: str='facebook/timesformer-base-finetuned-k400', num_classes: int=1):\n        super(HFModel, self).__init__()\n        self.hf_model = AutoModelForVideoClassification.from_pretrained(ckpt_name)\n        self.fc = nn.Linear(in_features=400, out_features=num_classes)\n\n    def forward(self, inputs):\n        x = self.hf_model(**inputs)\n        logits = x.logits\n        logits = self.fc(logits)\n        return logits\n","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:17:27.666986Z","iopub.execute_input":"2024-10-03T23:17:27.667634Z","iopub.status.idle":"2024-10-03T23:17:27.689536Z","shell.execute_reply.started":"2024-10-03T23:17:27.667571Z","shell.execute_reply":"2024-10-03T23:17:27.688740Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"import torch\n\nclass SaveBestModel:\n    def __init__(self, best_val_loss: float=float('inf'), save_path: str='../models_weights/best_model.pth'):\n        self.best_val_loss = best_val_loss\n        self.save_path = save_path\n\n    def __call__(self, model: torch.nn.Module, current_val_loss: float, prev_best_min_loss: float):\n        if current_val_loss < self.best_val_loss and current_val_loss < prev_best_min_loss:\n            self.best_val_loss = current_val_loss\n            torch.save(model, self.save_path)\n            print(f'Best model saved with Val Loss: {self.best_val_loss:.4f}')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:17:27.690658Z","iopub.execute_input":"2024-10-03T23:17:27.690981Z","iopub.status.idle":"2024-10-03T23:17:27.697401Z","shell.execute_reply.started":"2024-10-03T23:17:27.690947Z","shell.execute_reply":"2024-10-03T23:17:27.696259Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import precision_score, recall_score, f1_score\ndef calculate_metrics(y_true: List[int], y_pred: List[int]) -> Tuple[float]:\n    precision = precision_score(y_true, y_pred, average='weighted')\n    recall = recall_score(y_true, y_pred, average='weighted')\n    f1 = f1_score(y_true, y_pred, average='weighted')\n    accuracy = 100 * np.sum(y_true == y_pred) / len(y_true)\n    return accuracy, precision, recall, f1","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:17:27.698569Z","iopub.execute_input":"2024-10-03T23:17:27.698949Z","iopub.status.idle":"2024-10-03T23:17:27.707308Z","shell.execute_reply.started":"2024-10-03T23:17:27.698913Z","shell.execute_reply":"2024-10-03T23:17:27.706418Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import confusion_matrix\nimport torch.utils\nimport torch.utils.data\nfrom tqdm import tqdm\nfrom typing import List, Tuple\n\ndef evaluate(\n        dataloader: torch.utils.data.DataLoader, \n        model: torch.nn.Module, \n        criterion: torch.nn, \n        class_names: List[str]=None, \n        device: str=\"\", \n        is_testing: bool=False\n    ) -> Tuple[float]: \n\n    if device == \"\":\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    else:\n        device = torch.device(device)\n\n    model = model.to(device)\n    model.eval()\n    running_loss = 0.0\n    all_labels = []\n    all_predictions = []\n\n    with torch.no_grad():\n        for inputs, labels in tqdm(dataloader):\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            outputs = model(inputs)\n            outputs = outputs.squeeze(-1)\n            loss = criterion(outputs, labels)\n\n            running_loss += loss.item()\n\n            predicted = (torch.sigmoid(outputs) >= 0.5).long()\n            all_labels.extend(labels.cpu().numpy())\n            all_predictions.extend(predicted.cpu().numpy())\n\n    # Calculate average loss\n    avg_loss = running_loss / len(dataloader)\n\n    # Calculate evaluation metrics\n    accuracy, precision, recall, f1 = calculate_metrics(np.array(all_labels), np.array(all_predictions))\n\n    if is_testing:\n        conf_matrix = confusion_matrix(all_labels, all_predictions)\n        plt.figure(figsize=(8, 6))\n        sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n        plt.xlabel('Predicted Label')\n        plt.ylabel('True Label')\n        plt.title('Confusion Matrix')\n        plt.show()\n\n        print(f'Average Loss: {avg_loss:.4f}')\n        print(f'Accuracy: {accuracy:.4f}')\n        print(f'Precision: {precision:.4f}')\n        print(f'Recall: {recall:.4f}')\n        print(f'F1 Score: {f1:.4f}')\n\n    return avg_loss, accuracy, precision, recall, f1\n","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:17:27.708772Z","iopub.execute_input":"2024-10-03T23:17:27.709132Z","iopub.status.idle":"2024-10-03T23:17:27.820911Z","shell.execute_reply.started":"2024-10-03T23:17:27.709072Z","shell.execute_reply":"2024-10-03T23:17:27.819895Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"import torch\nimport numpy as np\nimport torch.utils\nimport torch.utils.data\nfrom tqdm import tqdm\nfrom torch.cuda.amp import autocast, GradScaler\n\ndef train(\n        model: torch.nn.Module, \n        train_loader: torch.utils.data.DataLoader, \n        val_loader: torch.utils.data.DataLoader, \n        criterion: torch.nn, \n        optimizer: torch.optim, \n        num_epochs: int=10, \n        device: str=\"\", \n        start_epoch: int=0, \n        scheduler: torch.optim.lr_scheduler=None, \n        new_lr: float=None,\n        models_weights_path: str='/kaggle/working/models_weights/best_model.pth',\n        prev_best_min_loss: float=float('inf'),\n        prev_history: dict=None\n    ) -> None:\n\n    if device == \"\":\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    # model = torch.compile(model, backend=\"nvfuser\")\n    dir_path = '/'.join(models_weights_path.split('/')[:-1])\n    os.makedirs(dir_path, exist_ok=True)\n    model = model.to(device)\n    save_best_model = SaveBestModel(save_path=models_weights_path)\n    if new_lr is not None:\n        for param_group in optimizer.param_groups:\n            param_group['lr'] = new_lr\n    train_losses = []\n    val_losses = []\n    train_accuracies = []\n    val_accuracies = []\n    \n    scaler = GradScaler()\n    for epoch in range(start_epoch, num_epochs):\n        print(f'Epoch {epoch+1}/{num_epochs}')\n        print('-' * 10)\n\n        # Training phase\n        model.train()\n        running_loss = 0.0\n        all_labels = []\n        all_predictions = []\n\n        for inputs, labels in tqdm(train_loader):\n            inputs, labels = inputs.to(device), labels.to(device)\n\n            optimizer.zero_grad()\n            with autocast():\n                outputs = model(inputs)\n                outputs = outputs.squeeze(-1)\n#                 print(outputs.shape)\n                loss = criterion(outputs, labels)\n\n            scaler.scale(loss).backward()\n            scaler.step(optimizer)\n            scaler.update()\n\n            running_loss += loss.item()\n\n            predicted = (torch.sigmoid(outputs) >= 0.5).long()\n            all_labels.extend(labels.cpu().numpy())\n            all_predictions.extend(predicted.cpu().numpy())\n\n        avg_train_loss = running_loss / len(train_loader)\n\n        train_losses.append(avg_train_loss)\n\n        train_accuracy, train_precision, train_recall, train_f1 = calculate_metrics(np.array(all_labels), np.array(all_predictions))\n        print(f'Train Loss: {avg_train_loss:.4f}, Accuracy: {train_accuracy:.2f}%, '\n              f'Precision: {train_precision:.4f}, Recall: {train_recall:.4f}, F1 Score: {train_f1:.4f}')\n\n        train_accuracies.append(train_accuracy)\n        # Validation phase\n        avg_val_loss, val_accuracy, val_precision, val_recall, val_f1 = evaluate(val_loader, model, criterion)\n\n        print(f'Val Loss: {avg_val_loss:.4f}, Accuracy: {val_accuracy:.2f}%, '\n              f'Precision: {val_precision:.4f}, Recall: {val_recall:.4f}, F1 Score: {val_f1:.4f}')\n        print(\"-\"*50)\n\n        val_losses.append(avg_val_loss)\n        val_accuracies.append(val_accuracy)\n        # Save the best model using the callback\n        save_best_model(model, avg_val_loss, prev_best_min_loss)\n\n        # Step the scheduler if it exists\n        if scheduler is not None:\n            scheduler.step(avg_val_loss if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau) else None)\n    print('Finished Training')\n    history = {\n        'train_loss': train_losses,\n        'val_loss': val_losses,\n        'train_accuracy': train_accuracies,\n        'val_accuracy': val_accuracies,\n        \"prev_best_min_loss\": save_best_model.best_val_loss\n    }\n    if prev_history is not None:\n        for key in prev_history.keys():\n            if key != 'prev_best_min_loss':\n                history[key] =  prev_history[key] + history[key]\n    return history","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:17:27.824950Z","iopub.execute_input":"2024-10-03T23:17:27.825576Z","iopub.status.idle":"2024-10-03T23:17:27.845167Z","shell.execute_reply.started":"2024-10-03T23:17:27.825536Z","shell.execute_reply":"2024-10-03T23:17:27.844187Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"code","source":"import torch.optim as optim\nfrom torch import nn\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n# device = torch.device('cpu')\nmodel = HFModel()\nmodel = model.to(device)\n# model = nn.DataParallel(model)\n# train_class_weights, normalized_class_weights = calculate_class_weights(train_files)\n# pos_weight = torch.tensor([train_class_weights[1]],dtype=torch.float32).to(device)\n# criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\ncriterion = nn.BCEWithLogitsLoss()\noptimizer = optim.AdamW(model.parameters(), lr=0.001)\nnum_epochs = 10\n\nhistory = train(model, train_dataloader, val_dataloader, criterion, optimizer, device=device, num_epochs=num_epochs, models_weights_path='/kaggle/working/models_weights/facebook-timesformer/best_model.pth')","metadata":{"execution":{"iopub.status.busy":"2024-10-03T23:18:50.211114Z","iopub.execute_input":"2024-10-03T23:18:50.212127Z","iopub.status.idle":"2024-10-04T00:38:04.659593Z","shell.execute_reply.started":"2024-10-03T23:18:50.212066Z","shell.execute_reply":"2024-10-04T00:38:04.658128Z"},"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"Epoch 1/10\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 275/275 [14:23<00:00,  3.14s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.7556, Accuracy: 51.27%, Precision: 0.5127, Recall: 0.5127, F1 Score: 0.5126\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [01:13<00:00,  1.15s/it]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.7087, Accuracy: 37.50%, Precision: 0.1406, Recall: 0.3750, F1 Score: 0.2045\n--------------------------------------------------\nBest model saved with Val Loss: 0.7087\nEpoch 2/10\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 275/275 [14:14<00:00,  3.11s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6970, Accuracy: 48.91%, Precision: 0.4891, Recall: 0.4891, F1 Score: 0.4890\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [01:13<00:00,  1.14s/it]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.6893, Accuracy: 62.50%, Precision: 0.3906, Recall: 0.6250, F1 Score: 0.4808\n--------------------------------------------------\nBest model saved with Val Loss: 0.6893\nEpoch 3/10\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 275/275 [14:16<00:00,  3.11s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6963, Accuracy: 48.18%, Precision: 0.4818, Recall: 0.4818, F1 Score: 0.4818\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [01:12<00:00,  1.13s/it]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.6973, Accuracy: 37.50%, Precision: 0.1406, Recall: 0.3750, F1 Score: 0.2045\n--------------------------------------------------\nEpoch 4/10\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 275/275 [14:14<00:00,  3.11s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6966, Accuracy: 47.82%, Precision: 0.4781, Recall: 0.4782, F1 Score: 0.4777\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [01:12<00:00,  1.13s/it]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.6882, Accuracy: 62.50%, Precision: 0.3906, Recall: 0.6250, F1 Score: 0.4808\n--------------------------------------------------\nBest model saved with Val Loss: 0.6882\nEpoch 5/10\n----------\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 275/275 [14:14<00:00,  3.11s/it]\n","output_type":"stream"},{"name":"stdout","text":"Train Loss: 0.6961, Accuracy: 48.18%, Precision: 0.4814, Recall: 0.4818, F1 Score: 0.4792\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 64/64 [01:12<00:00,  1.13s/it]\n","output_type":"stream"},{"name":"stdout","text":"Val Loss: 0.6848, Accuracy: 62.50%, Precision: 0.3906, Recall: 0.6250, F1 Score: 0.4808\n--------------------------------------------------\nBest model saved with Val Loss: 0.6848\nEpoch 6/10\n----------\n","output_type":"stream"},{"name":"stderr","text":" 12%|█▏        | 33/275 [01:41<12:20,  3.06s/it]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[26], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdamW(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m     13\u001b[0m num_epochs \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m---> 15\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_epochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodels_weights_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/kaggle/working/models_weights/facebook-timesformer/best_model.pth\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[24], line 50\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, start_epoch, scheduler, new_lr, models_weights_path, prev_best_min_loss, prev_history)\u001b[0m\n\u001b[1;32m     47\u001b[0m all_labels \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     48\u001b[0m all_predictions \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader):\n\u001b[1;32m     51\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     53\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/tqdm/std.py:1181\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1180\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1181\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1182\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 630\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    633\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:673\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    671\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    672\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 673\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    674\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    675\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n","Cell \u001b[0;32mIn[15], line 34\u001b[0m, in \u001b[0;36mVideoDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     32\u001b[0m video \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_frames(video_path)\n\u001b[1;32m     33\u001b[0m frame_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(video)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_frames, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m video \u001b[38;5;241m=\u001b[39m [Image\u001b[38;5;241m.\u001b[39mfromarray(frame\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muint8\u001b[39m\u001b[38;5;124m'\u001b[39m)) \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m video]\n\u001b[1;32m     35\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnon shop lifters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m video_path \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1.\u001b[39m\n\u001b[1;32m     37\u001b[0m video \u001b[38;5;241m=\u001b[39m [video[frame_idx] \u001b[38;5;28;01mfor\u001b[39;00m frame_idx \u001b[38;5;129;01min\u001b[39;00m frame_indices]\n","Cell \u001b[0;32mIn[15], line 34\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     32\u001b[0m video \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextract_frames(video_path)\n\u001b[1;32m     33\u001b[0m frame_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(video)\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_frames, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m---> 34\u001b[0m video \u001b[38;5;241m=\u001b[39m [\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43muint8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m video]\n\u001b[1;32m     35\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnon shop lifters\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m video_path \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m1.\u001b[39m\n\u001b[1;32m     37\u001b[0m video \u001b[38;5;241m=\u001b[39m [video[frame_idx] \u001b[38;5;28;01mfor\u001b[39;00m frame_idx \u001b[38;5;129;01min\u001b[39;00m frame_indices]\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3304\u001b[0m, in \u001b[0;36mfromarray\u001b[0;34m(obj, mode)\u001b[0m\n\u001b[1;32m   3301\u001b[0m         msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstrides\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m requires either tobytes() or tostring()\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   3302\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m-> 3304\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrombuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mraw\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrawmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3206\u001b[0m, in \u001b[0;36mfrombuffer\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   3203\u001b[0m         im\u001b[38;5;241m.\u001b[39mreadonly \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   3204\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m im\n\u001b[0;32m-> 3206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrombytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3137\u001b[0m, in \u001b[0;36mfrombytes\u001b[0;34m(mode, size, data, decoder_name, *args)\u001b[0m\n\u001b[1;32m   3112\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3113\u001b[0m \u001b[38;5;124;03mCreates a copy of an image memory from pixel data in a buffer.\u001b[39;00m\n\u001b[1;32m   3114\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3132\u001b[0m \u001b[38;5;124;03m:returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[1;32m   3133\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3135\u001b[0m _check_size(size)\n\u001b[0;32m-> 3137\u001b[0m im \u001b[38;5;241m=\u001b[39m \u001b[43mnew\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m im\u001b[38;5;241m.\u001b[39mwidth \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m im\u001b[38;5;241m.\u001b[39mheight \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   3139\u001b[0m     decoder_args: Any \u001b[38;5;241m=\u001b[39m args\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/PIL/Image.py:3102\u001b[0m, in \u001b[0;36mnew\u001b[0;34m(mode, size, color)\u001b[0m\n\u001b[1;32m   3100\u001b[0m         im\u001b[38;5;241m.\u001b[39mpalette \u001b[38;5;241m=\u001b[39m ImagePalette\u001b[38;5;241m.\u001b[39mImagePalette()\n\u001b[1;32m   3101\u001b[0m         color \u001b[38;5;241m=\u001b[39m im\u001b[38;5;241m.\u001b[39mpalette\u001b[38;5;241m.\u001b[39mgetcolor(color_ints)\n\u001b[0;32m-> 3102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m im\u001b[38;5;241m.\u001b[39m_new(\u001b[43mcore\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfill\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolor\u001b[49m\u001b[43m)\u001b[49m)\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"history","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model = torch.load('/kaggle/working/models_weights/facebook-timesformer/best_model.pth')\nevaluate(test_dataloader, best_model, criterion, class_names=['Non Shop Lifters', 'Shop Lifters'], device='cuda', is_testing=True)","metadata":{"execution":{"iopub.status.busy":"2024-10-04T00:38:53.418309Z","iopub.execute_input":"2024-10-04T00:38:53.418742Z","iopub.status.idle":"2024-10-04T00:40:06.257804Z","shell.execute_reply.started":"2024-10-04T00:38:53.418702Z","shell.execute_reply":"2024-10-04T00:40:06.256867Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"100%|██████████| 65/65 [01:12<00:00,  1.11s/it]\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 800x600 with 2 Axes>","image/png":"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"},"metadata":{}},{"name":"stdout","text":"Average Loss: 0.6855\nAccuracy: 61.5385\nPrecision: 0.3787\nRecall: 0.6154\nF1 Score: 0.4689\n","output_type":"stream"},{"execution_count":29,"output_type":"execute_result","data":{"text/plain":"(0.6854593977332115,\n 61.53846153846154,\n 0.37869822485207105,\n 0.6153846153846154,\n 0.4688644688644689)"},"metadata":{}}]}]}